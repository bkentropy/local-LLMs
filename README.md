# Working with Local LLMS
This repo is just for education purposes. It's meant to get to you started by working with local llms.

# Getting Started 
First create your environment
```
uv sync
```

Activate your environment
```
.venv\Scripts\Activate.ps1
```

Download the model
```
ollama pull llama3.1:8b
```

Run the example
```
python chat_with_llama.py
```